{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZH6d1OspMZwY+SMm3RLH3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NajlaaNawaii/Math-and-Machine-Learning-Puzzles/blob/main/ML%20Code%20Challenges/ML_Code_Challenges_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning Easy!"
      ],
      "metadata": {
        "id": "4HAGylrXJ112"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sigmoid Activation Function Understanding (easy) ✔\n",
        "Write a Python function that computes the output of the sigmoid activation function given an input value z. The function should return the output rounded to four decimal places."
      ],
      "metadata": {
        "id": "mKep4IscKDtM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B10_1Y_gJBeD"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def sigmoid(z: float) -> float:\n",
        "\t#Your code here\n",
        "\tresult= 1 / (1 + math.exp(-z))\n",
        "\treturn round(result,4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Softmax Activation Function Implementation (easy) ✔\n",
        "Write a Python function that computes the softmax activation for a given list of scores. The function should return the softmax values as a list, each rounded to four decimal places."
      ],
      "metadata": {
        "id": "pOI9bF8ONUxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def softmax(scores: list[float]) -> list[float]:\n",
        "\t# Your code here\n",
        "\tintermediate= np.exp(scores)\n",
        "\tsummation=sum(intermediate)\n",
        " #with using numpy, it gives output vector\n",
        "\tprobabilities=intermediate/summation\n",
        "\treturn probabilities"
      ],
      "metadata": {
        "id": "x3-2M1N7Ld3f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Single Neuron (easy) ✔\n",
        "Write a Python function that simulates a single neuron with a sigmoid activation function for binary classification, handling multidimensional input features. The function should take a list of feature vectors (each vector representing multiple features for an example), associated true binary labels, and the neuron's weights (one for each feature) and bias as input. It should return the predicted probabilities after sigmoid activation and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places."
      ],
      "metadata": {
        "id": "1HcqfBXZS5Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n",
        "\t# Your code here\n",
        "\tfeatures= np.array(features)\n",
        "\tweights=np.array(weights)\n",
        "\tscores= (features @ weights )+ bias\n",
        "\tsigmoided=1/(1+np.exp(-scores))\n",
        "\n",
        "\tmse =  sum((sigmoided - labels)**2) / len(features)\n",
        "\treturn sigmoided , mse"
      ],
      "metadata": {
        "id": "l7FU30qPS2bO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]]\n",
        "labels = [0, 1, 0]\n",
        "weights = [0.7, -0.4]\n",
        "bias = -0.1\n",
        "\n",
        "single_neuron_model(features, labels, weights, bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebxt4oovere9",
        "outputId": "a2d39a84-8b2e-443c-949b-602f6aa46984"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.46257015, 0.41338242, 0.66818777]), 0.33485541024953136)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementation of Log Softmax Function ✔\n",
        "In machine learning and statistics, the softmax function is a generalization of the logistic function that converts a vector of scores into probabilities. The log-softmax function is the logarithm of the softmax function, and it is often used for numerical stability when computing the softmax of large numbers. Given a 1D numpy array of scores, implement a Python function to compute the log-softmax of the array."
      ],
      "metadata": {
        "id": "OPau1LsDekgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def log_softmax(scores: list) -> np.ndarray:\n",
        "\t# Your code here\n",
        "\tscores=np.array(scores)\n",
        "\tintermediate=np.exp(scores)\n",
        "\tsoftmax=intermediate / sum(intermediate)\n",
        "\tlog_softmax=np.log(softmax)\n",
        "\treturn log_softmax"
      ],
      "metadata": {
        "id": "TJHsPtKGejLO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implement ReLU Activation Function ✔\n",
        "Write a Python function `relu` that implements the Rectified Linear Unit (ReLU) activation function. The function should take a single float as input and return the value after applying the ReLU function. The ReLU function returns the input if it's greater than 0, otherwise, it returns 0."
      ],
      "metadata": {
        "id": "zRAjDPQ0im9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z: float) -> float:\n",
        "\t# Your code here\n",
        "\treturn max(0,z)\n"
      ],
      "metadata": {
        "id": "05bbFxWYqJt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Leaky ReLU Activation Function ✔\n",
        "Write a Python function `leaky_relu` that implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function. The function should take a float `z` as input and an optional float `alpha`, with a default value of 0.01, as the slope for negative inputs. The function should return the value after applying the Leaky ReLU function."
      ],
      "metadata": {
        "id": "B7dhF6CukugT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu(z: float, alpha: float = 0.01) -> float|int:\n",
        "\t# Your code here\n",
        "\treturn z if z > 0 else alpha*z\n"
      ],
      "metadata": {
        "id": "7BH64CfUktnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##KL Divergence Between Two Normal Distributions ✔\n",
        "\n",
        "Write a function kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q) that calculates the KL divergence between two normal distributions."
      ],
      "metadata": {
        "id": "bVneysEzqkNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n",
        "\tterm_1=np.log(sigma_q / sigma_p)\n",
        "\tterm_2=(sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n",
        "\tKL_value= term_1 + term_2 - .5\n",
        "\treturn KL_value\n"
      ],
      "metadata": {
        "id": "0NxZ-1Ydqiop"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9x8doaq_q3oV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}